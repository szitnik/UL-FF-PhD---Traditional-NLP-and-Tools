{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional language models\n",
    "\n",
    "<sup>This notebook is a part of Natural Language Processing class at the University of Ljubljana, Faculty for computer and information science. Please contact [slavko.zitnik@fri.uni-lj.si](mailto:slavko.zitnik@fri.uni-lj.si) for any comments.</sub>\n",
    "\n",
    "A model (from statistical point of view) is a mathematical representation of a process. Models may be an approximation of a process and there are two important reasons for this: \n",
    "\n",
    "1. We observe the process a limited amount of times.\n",
    "2. A model can be very complex so we should normally simplify it.\n",
    "\n",
    "In statistics we may have heard: `All models are wrong, but some are useful.`\n",
    "\n",
    "## Bag of words\n",
    "\n",
    "We have already seen some models. One of them and also the simplest one is bag-of-words model, which is a naive way of modelling human language. But still, it is useful and popular. For the bag-of-words model we also know: \n",
    "\n",
    "1. It has an oversimplified view of the language.\n",
    "2. It takes into account only the frequency of the words in the language, not their order or position.\n",
    "3. In a way we have created it, it was useful for tasks such as text classification or sentiment analysis, where we were interested only into separate words and their count.\n",
    "\n",
    "## n-Grams\n",
    "\n",
    "Text is always a sequence - a sequence of words, characters, symbols, ... So one idea might be to model how text is generated or which token is most probably to proceed in a given sequence. We can learn probabilities over two tokens (bigrams), three tokens (trigrams), ... n tokens (n-grams).\n",
    "\n",
    "\"Bigram\" is just a fancy name for 2 consecutive words while n-gram is an n-tuple of consecutive tokens. Let's show a quick example of using word-based n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence: \n",
      "\t['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.']\n",
      "\n",
      "\n",
      "Bigrams: \n",
      "\t[('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.')]\n",
      "\n",
      "\n",
      "Padded bigrams: \n",
      "\t[(None, 'ASIAN'), ('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.'), ('.', None)]\n",
      "\n",
      "\n",
      "Trigrams: \n",
      "\t[('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.')]\n",
      "\n",
      "\n",
      "Padded trigrams: \n",
      "\t[(None, None, 'ASIAN'), (None, 'ASIAN', 'EXPORTERS'), ('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.'), ('said', '.', None), ('.', None, None)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from functools import reduce\n",
    "import operator\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    " \n",
    "first_sentence = reuters.sents()[0]\n",
    "print(\"First sentence: \\n\\t{}\\n\\n\".format(first_sentence))\n",
    " \n",
    "print(\"Bigrams: \\n\\t{}\\n\\n\".format(list(bigrams(first_sentence))))\n",
    "print(\"Padded bigrams: \\n\\t{}\\n\\n\".format(list(bigrams(first_sentence, pad_left=True, pad_right=True))))\n",
    "print(\"Trigrams: \\n\\t{}\\n\\n\".format(list(trigrams(first_sentence))))\n",
    "print(\"Padded trigrams: \\n\\t{}\\n\\n\".format(list(trigrams(first_sentence, pad_left=True, pad_right=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'What the economists' trigram occurence number: 2\n",
      "'Hell' follows 'What the' in 0 cases.\n",
      "8839 sentences start with 'The'\n",
      "\n",
      "\n",
      "'What the economists' trigram probability in given text: 0.0435\n",
      "The probability of 'Hell' following 'What the' is 0.0.\n",
      "The probability of a sentence to start with 'The' is 0.162.\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the data and count occurences\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    " \n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "\n",
    "print(\"'What the economists' trigram occurence number: {}\".format(model[\"what\", \"the\"][\"economists\"]))\n",
    "print(\"'Hell' follows 'What the' in {} cases.\".format(model[\"what\", \"the\"][\"hell\"]))\n",
    "print(\"{} sentences start with 'The'\\n\\n\".format(model[None, None][\"The\"])) \n",
    "\n",
    "# 2. transform occurences to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n",
    "\n",
    "print(\"'What the economists' trigram probability in given text: {:.3}\".format(model[\"what\", \"the\"][\"economists\"]))\n",
    "print(\"The probability of 'Hell' following 'What the' is {:.3}.\".format(model[\"what\", \"the\"][\"hell\"]))\n",
    "print(\"The probability of a sentence to start with 'The' is {:.3}.\".format(model[None, None][\"The\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation\n",
    "\n",
    "#### Greedy approach\n",
    "Algorithm: Select the most probable word given last n-1 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of text: 0.0034298705414246475\n",
      "\n",
      "Token probabilities: \n",
      "\t'0.16154324146501936 0.13055775540219483 0.6303797468354431 0.2580115036976171 0.9998732251521298 1.0'\n",
      "Generated sequence: \n",
      "\t'The company said .'\n"
     ]
    }
   ],
   "source": [
    "# 3. Use the model (e.g. text generation)\n",
    "text = [None, None]\n",
    "sentence_finished = False\n",
    "probs = []\n",
    "\n",
    "while not sentence_finished:\n",
    "    token = max(model[tuple(text[-2:])].items(), key=operator.itemgetter(1))\n",
    "    text.append(token[0])\n",
    "    probs.append(token[1])\n",
    "\n",
    "    if text[-2:] == [None, None]:\n",
    "        break\n",
    "\n",
    "print(f\"Probability of text: {reduce(operator.mul, probs, 1)}\\n\")\n",
    "print(f\"Token probabilities: \\n\\t'{' '.join([str(prob) for prob in probs if token])}'\")\n",
    "print(f\"Generated sequence: \\n\\t'{' '.join([token for token in text if token])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the example above **greedy decoding** which will always yied the same result given input. To generate more useful text, tokens should be selected more random and taking into account cumulative scores. For example, there are some options:\n",
    "\n",
    "* **Beam search** is also a deterministic decoding and offers an improvement over greedy decoding. A problem of greedy decoding is that we might miss the most likely sequence since we predict only the most probable word at each timestep. Beam search mitigates this by keeping a track of most probable n sequences at every step and ultimately selecting the most probable sequence.\n",
    "* **Top *k* sampling** selects k most probable words and distributes their comulative probability over them. The problem is that we must choose a fixed sized parameter k which might lead to suboptimal results in some scenarios.\n",
    "* **Top *p* sampling** addresses this by selecting top words whose cumulative probability just exceeds p. This comulative probability is then again distributed among these words.\n",
    "\n",
    "A randomized example below, which shows there are no rules and you might use your imagination in NLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use the model (e.g. text generation)\n",
    " \n",
    "def rand_ngram_generator(initial):\n",
    "    text = initial\n",
    "    sentence_finished = False\n",
    "    prob = 1.0\n",
    "\n",
    "    while not sentence_finished:\n",
    "        r = random.random()\n",
    "        accumulator = .0\n",
    "\n",
    "        for word in model[tuple(text[-2:])].keys():\n",
    "            accumulator += model[tuple(text[-2:])][word] \n",
    "\n",
    "            if accumulator >= r:\n",
    "                prob *= model[tuple(text[-2:])][word]\n",
    "                text.append(word)\n",
    "                break\n",
    "\n",
    "        if text[-2:] == [None, None]:\n",
    "            sentence_finished = True\n",
    "\n",
    "    print(f\"Probability of text: {prob}\\n\")\n",
    "    print(f\"Generated sequence: \\n\\t'{' '.join([token for token in text if token])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of text: 8.879748706453449e-07\n",
      "\n",
      "Generated sequence: \n",
      "\t'Sumita says Bank of Japan .'\n"
     ]
    }
   ],
   "source": [
    "rand_ngram_generator([None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of text: 7.892981377685451e-120\n",
      "\n",
      "Generated sequence: \n",
      "\t'I am deeply concerned over the ships , but took no action on the company ' s Minister of State for Finance and Petroleum Minister Sheikh Ali al - Bukhoosh was producing 57 , 000 vs 17 , 552 , 000 vs profit 37 cts Net loss 946 , 024 , 000 dlrs , respectively and extraordinary income of 10 kwacha per dollar on condition that WCAS ' affiliate investors would commit 50 mln dlrs for capital goods , local and international bankers as steps in the financial year and many countries to reach 403 dlrs per tonne FOB , plus a contingent , variable import tariff cuts and many factories have gone up for 50 or 60 mln dlrs this year , bad weather has already had ,\" one source said India has given the reduced significance of the yen ' s name would be appropriate .'\n"
     ]
    }
   ],
   "source": [
    "rand_ngram_generator([\"I\", \"am\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of text: 2.2535062065745263e-29\n",
      "\n",
      "Generated sequence: \n",
      "\t'They were the main rainy season , compared to 2 . 5 pct against a market inaugurated by the U . S . Silas said he lowered its stake in it .'\n"
     ]
    }
   ],
   "source": [
    "rand_ngram_generator([\"They\", \"were\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Information extraction (unsupervised approaches)\n",
    "\n",
    "#### Open information(/relation) extraction\n",
    "\n",
    "As for most of the tasks we need classification into predefined classes, a pure unsupervised end-to-end information extraction approach would use Open information extraction. The latter means that a method extracts tuples of type (subject, predicate, object) without classification of any of them to a list of predefined classes. \n",
    "\n",
    "These methods mostly use some technique of bootstrapping or semi-supervised approach to define lexical rules for extraction. As it may seem as an easy task, it is hard to retrieve clean examples and also evaluate the results.\n",
    "\n",
    "The area became popular with the introduction of the *TextRunner* system in 2007, after which new systems were introduced. The same research group released the latest version, called [Open IE 5.0](https://github.com/dair-iitd/OpenIE-standalone) in the beginning of 2017. Results of such systems are very useful for QA systems.\n",
    "\n",
    "### Information extraction (supervised approaches)\n",
    "\n",
    "#### Named entity recognition\n",
    "\n",
    "Named entity recognition is a sequence labeling task. The goal of the algorithm is to define a specific class for each token in a sequence (see sequence labeling lab session for examples).\n",
    "\n",
    "#### Coreference resolution\n",
    "\n",
    "Coreference resolution is a task of mention clustering. It basically consists of the following:\n",
    "\n",
    "1. Mention identification.\n",
    "2. Mention clustering. \n",
    "\n",
    "Mentions refer to underlying entities and are of named, nominal or pronominal type.\n",
    "\n",
    "There exist many supervised or unsupervised approaches to coreference resolution. One of the most known approached has been `Sieve-based coreference resolution` system by the Stanford NLP group, which achieves comparable state-of-the art results by only employing lexically predefined rules.\n",
    "\n",
    "Python bindings to a recent state-of-the-art system is available in a [public source code repository](https://github.com/huggingface/neuralcoref) along with a [web-based example](https://huggingface.co/coref/).\n",
    "\n",
    "#### Relation(ship) extraction\n",
    "\n",
    "Relationship extraction or relation extraction is another information extraction task in which the idea is to idetify relationships between mentions or text spans. The task of true relationship extraction consists of:\n",
    "\n",
    "1. Subject and object identification.\n",
    "2. Relationship identification and extraction.\n",
    "\n",
    "The area of relationships extraction is also very broad and is sometimes related to ontology extraction or building. As there may exist a number of different relationships in text, only some tagged datasets exist, which contain a few basic relationships. Apart from general relationship extraction, the task has became popular mostly because of biological relationships extraction (interactions between genes and proteins).\n",
    "\n",
    "An example of a successful relationship extraction system that is using neural networks was presented at the EMNLP 2017 and is accessible in the [public source code repository](https://github.com/UKPLab/emnlp2017-relation-extraction).\n",
    "\n",
    "### Other approaches\n",
    "\n",
    "Apart from fully supervised or unsupervised there exist semi-supervised approaches and manual knowledge base engineering. Some of the knowledge bases contain a predefined schema with rules following Semantic Web principles. The latter enables graph-based data representation within Linked Open Data, easy interconnection of databases and structured querying using SPARQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Currently, ***COMMON-SENSE REASONING KNOWLEDGE BASES*** are popular ;). This is just another marketing term, similar as ***AI*** :). \n",
    "\n",
    "One of the recent databases developed by AllenNLP institute is called ATOMIC. It contains 887.000 descriptions of reasoning over 300.000 events. The descriptions are provided using IF-THEN rules for 9 types of relationships (Figure below). \n",
    "\n",
    "![](ATOMIC.png)\n",
    "\n",
    "Over ATOMIC a \"reasoning\" tool COMeT (Bosselut in sod., 2019) was developed using GPT algorithm. It is available online via [Mosaic Web application](https://mosaickg.apps.allenai.org/comet_atomic).\n",
    "\n",
    "Slovene version is available at [https://multicomet.ijs.si](https://multicomet.ijs.si).\n",
    "\n",
    "Other examples of knowledge bases: \n",
    "* ConceptNet\n",
    "* Web Child\n",
    "* Wikidata\n",
    "* WordNet\n",
    "* Roget\n",
    "* VerbNet\n",
    "* FrameNet\n",
    "* VisualGenome\n",
    "* ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Collect as many as possible Christmas and New year wishes (in Slovene). Then analyse your corpus and train a simple language model that would generate a wish for your close ones.\n",
    "\n",
    "Implement beam search, different sampling techniques and compare results. Use some other and larger corpora.\n",
    "\n",
    "Think of possible usages of a knowledge base and search for them. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
